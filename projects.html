<!DOCTYPE HTML>
<!--
	Spatial by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
		<title>Lukas Muttenthaler | Brains & Machines</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body>

		<!-- Header -->
			<header id="header">
				<h1><strong><a href="index.html">Projects</a></strong></h1>
				<nav id="nav">
					<ul>
						<li><a href="index.html">About Me</a></li>
						<li><a href="projects.html">Projects</a></li>
						<li><a href="publications.html">Publications</a></li>
						<li><a href="blog.html">Blog</a></li>
					</ul>
				</nav>
			</header>

			<a href="#menu" class="navPanelToggle"><span class="fa fa-bars"></span></a>

		<!-- Main -->
			<section id="main" class="wrapper">
				<div class="container">

					<header class="major special">
						<h2>Current Projects</h2>
					</header>
					<!-- Text -->
						<section>
							<h3>Thingsvision</h3>
							<p>I've developed a pipeline to easily extract hidden unit activations at any layer for any image from an array of state-of-the-art <a href="https://pytorch.org/docs/stable/torchvision/models.html">torchvision models</a>.
							The user is only required to specify the PATH where the images can be found on disk, the desired vision model, the layer, whether activations should be flattened (tensor2vectors) or extracted as is (e.g., as multidimensional arrays), 
							and in which file format (e.g., .npy) the hidden unit activations should be stored. The GitHub repository contains a Python dataset class that converts the user's images into a read-to-use <a href="https://pytorch.org/">PyTorch</a> dataset,
							which is subsequently wrapped with a mini-batch dataloader to make neural activation extraction fast and efficient. Activations are automatically stored at the specified location, where they can be found after extraction. 
							Have a look at the GitHub repo for more details about implementation and use.<br>
							<a href="https://github.com/ViCCo-Group/thingsvision">[code]</a> </p>
							<hr />

					<header class="major special">
							<h2>Previous Projects</h2>
					</header>
					<!-- Text -->
						<section>
							<h3>Subjective Question Answering (QA)</h3>
							
							<p>Humans ask Google (and other search engines) an array of questions. Frequently, those questions address subjective opinions. 
							People want to know where to find a romantic hotel, the best Italian pizza place, or the hippest bar in town.
							Therefore, it is crucial that algorithms are capable of accurately answering not solely factual but also subjective questions. 
							Together with <a href="http://bjerva.github.io/">Johannes Bjerva</a> and <a href="http://isabelleaugenstein.github.io/">Isabelle Augenstein</a>
						 	I tried to resolve this conundrum by looking into different neural architectures as part of my Master's thesis. </p>
							
							<p> I've further developed a method to predict whether an answer to such a subjective question is correct or not, solely by looking 
							into the token representations of the neural network (at each of its layers) prior to predicting the start and end positions of the answer span.
							This "follow-up" work (~"alongside" work) was accepted for publication in the Proceedings of the <a href="https://2020.emnlp.org/">2020 EMNLP</a> workshop <i><a href="https://blackboxnlp.github.io/">BlackboxNLP</a>: Analyzing and Interpreting Neural Networks for NLP</i>.<br>
							<a href="https://github.com/LukasMut/Subjective_QA">[code]</a> &nbsp; <a href="https://www.aclweb.org/anthology/2020.blackboxnlp-1.8/">[paper]</a> &nbsp; </a> <a href="https://arxiv.org/abs/2006.08342">[Master's thesis]</a> </p>
							<hr />
		
							<h3>Compositional skills of Recurrent Neural Networks (RNNs)</h3>

							<p>Reimplementation of all experiments in the Facebook AI Research paper by <a href="https://arxiv.org/abs/1711.00350">  Lake, B. and Baroni, M. (2018) </a> and improvement on those.
							Implementation of various Encoder-Decoder RNNs, LSTMs and GRUs both with and without attention to investigate compositional skills of sequence-to-sequence models.
							Contrary to Lake, B. and Baroni, M. (2018), we implemented mini-batch training to conduct experiments computationally more efficiently. 
							Furthermore, we implemented a Teacher Forcing (TF) scheduler that gradually decreases the TF ratio over epochs to better prepare the model for inference time. <br>
							<a href="https://github.com/LukasMut/ATNLP">[code]</a> &nbsp; <a href="documents/paper/Compositional_Skills_of_Recurrent_Neural_Networks.pdf">[paper]</a> </p>
							<hr />

							<h3>Human brain activity for machine attention</h3>

							<p>Investigating with <a href="https://norahollenstein.github.io/"> Nora Hollenstein</a> and <a href="https://di.ku.dk/english/staff/?pure=en/persons/455610"> Maria Barrett </a> 
							how different reading tasks, and hence their respective cognitive loads, affect human brain activity, and how we can leverage this activity to embed words in EEG space.
							Moreover, we scrutinized whether informing machine attention with EEG activity from human readers helps neural models to process natural language.
							In this endeavour, we exploited the recently developed <a href="https://osf.io/q3zws/"> ZuCo corpus</a> for which I am a contributor.<br>
							<a href="https://github.com/LukasMut/NER-with-EEG-and-ET">[code]</a> &nbsp; <a href="https://arxiv.org/pdf/2006.05113.pdf">[paper]</a> </p>

							<hr />

							<h3>Text-to-SQL with BERT</h3>

							<p>Investigated how questions in the form of natural language (NL) can be mapped to a machine readable (i.e., logical) form, namely SQL, to query a database. This task is also called semantic parsing.<br>
							<a href="https://github.com/LukasMut/TypeSQL-plus-BERT">[code]</a> &nbsp; <a href="documents/paper/Text2SQL_LukasMuttenthaler.pdf">[paper]</a></p>

							<hr />

							<h3>Emoji Prediction</h3>

							<p>What's the last emoji? The use of emojis is more frequent than ever due to both the rapid growth of social media platforms and the increasing amount of available emojis in the virtual world.
							That's why, <a href="https://www.linkedin.com/in/gordon-lucas-a622a2154/"> Gordon </a> and I thought it would be a fun task to predict emojis in Twitter tweets given the words people use in their tweets.<br>
							<a href="https://github.com/Killiango/Emoji-Prediction">[code]</a></p>

							<hr />

							<h3>Cross-Domain Authorship Attribution</h3>

							<p>Authorship Attribution is the task of determining the author of a text from a set of candidates. This so-called classification task (multi-class at most times) is highly relevant for fields such as forensics
							or the detection of plagiarism. Here, we were classifying authors of fan-fictional texts such as J.K. Rowling copycats as part of the PAN competition at CLEF 2019.<br>
							<a href="http://ceur-ws.org/Vol-2380/paper_264.pdf">[paper about competition]</a> &nbsp; <a href="http://ceur-ws.org/Vol-2380/paper_49.pdf">[our paper]</a> &nbsp; <a href="https://github.com/LukasMut/AuthorshipAttribution">[code]</a></p>

							<hr />
									
					<header class="major special">
						<h2>Tutorials</h2>
					</header>
					<!-- Text -->
						<section>
							<h3>ResNets with PyTorch</h3>
							
							<p>A long time ago (at the beginning of my Master's), I've created a little tutorial on Residual Networks (ResNets) in PyTorch,
							and how to use a couple of convenient PyTorch functions and objects.
							If you, like me back then, belong to the PyTorch newbies, this might help you to start.<br>
							<a href="https://github.com/LukasMut/ResNet-in-PyTorch">[tutorial]</a></p>
							
							<hr />

					
				</div>
			</section>
					
		<!-- Footer -->
		<footer id="footer">
			<div class="container">
				<ul class="icons">
					<li><a href="https://github.com/LukasMut" class="icon fa-github"></a></li>
					<li><a href="https://twitter.com/lukas_mut" class="icon fa-twitter"></a></li>
					<li><a href="https://www.linkedin.com/in/lukas-muttenthaler/" class="icon fa-linkedin"></a></li>
				</ul>
				<ul class="copyright">
						<li>&copy; Lukas Muttenthaler</li>
						<li>Design: <a href="http://templated.co">TEMPLATED</a></li>
						<li>Images: <a href="http://unsplash.com">Unsplash</a></li>
				</ul>
			</div>
		</footer>
	</body>
</html>
